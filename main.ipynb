{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2957bcb9",
   "metadata": {},
   "source": [
    "# Algotihms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f1e82",
   "metadata": {},
   "source": [
    "All policies consists of Feature extraction and a FNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d39b7",
   "metadata": {},
   "source": [
    "BaseFeatureExtractor is the base class for all other Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f14d92",
   "metadata": {},
   "source": [
    "stable_baselines3.common.policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd7251",
   "metadata": {},
   "source": [
    "['ABC',\n",
    " 'ActorCriticCnnPolicy', == stable_baselines3.ppo.CnnPolicy\n",
    " 'ActorCriticPolicy', == stable_baselines3.ppo.MlpPolicy\n",
    " 'Any',\n",
    " 'BaseFeaturesExtractor',\n",
    " 'BaseModel',\n",
    " 'BasePolicy',\n",
    " 'BernoulliDistribution',\n",
    " 'CategoricalDistribution',\n",
    " 'CombinedExtractor',\n",
    " 'ContinuousCritic',\n",
    " 'DiagGaussianDistribution',\n",
    " 'Dict',\n",
    " 'Distribution',\n",
    " 'FlattenExtractor',\n",
    " 'List',\n",
    " 'MlpExtractor',\n",
    " 'MultiCategoricalDistribution',\n",
    " 'MultiInputActorCriticPolicy', == stable_baselines3.ppo.MultiInputPolicy\n",
    " 'NatureCNN',\n",
    " 'Optional',\n",
    " 'Schedule',\n",
    " 'SelfBaseModel',\n",
    " 'StateDependentNoiseDistribution',\n",
    " 'Tuple',\n",
    " 'Type',\n",
    " 'TypeVar',\n",
    " 'Union',\n",
    " '__builtins__',\n",
    " '__cached__',\n",
    " '__doc__',\n",
    " '__file__',\n",
    " '__loader__',\n",
    " '__name__',\n",
    " '__package__',\n",
    " '__spec__',\n",
    " 'abstractmethod',\n",
    " 'collections',\n",
    " 'copy',\n",
    " 'create_mlp',\n",
    " 'get_action_dim',\n",
    " 'get_device',\n",
    " 'is_image_space',\n",
    " 'is_vectorized_observation',\n",
    " 'make_proba_distribution',\n",
    " 'maybe_transpose',\n",
    " 'nn',\n",
    " 'np',\n",
    " 'obs_as_tensor',\n",
    " 'partial',\n",
    " 'preprocess_obs',\n",
    " 'spaces',\n",
    " 'th',\n",
    " 'warnings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e95c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ActorCriticPolicy',\n",
       " 'Any',\n",
       " 'BaseAlgorithm',\n",
       " 'BaseCallback',\n",
       " 'Dict',\n",
       " 'DictRolloutBuffer',\n",
       " 'GymEnv',\n",
       " 'List',\n",
       " 'MaybeCallback',\n",
       " 'OnPolicyAlgorithm',\n",
       " 'Optional',\n",
       " 'RolloutBuffer',\n",
       " 'Schedule',\n",
       " 'SelfOnPolicyAlgorithm',\n",
       " 'Tuple',\n",
       " 'Type',\n",
       " 'TypeVar',\n",
       " 'Union',\n",
       " 'VecEnv',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'np',\n",
       " 'obs_as_tensor',\n",
       " 'safe_mean',\n",
       " 'spaces',\n",
       " 'sys',\n",
       " 'th',\n",
       " 'time']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir(stable_baselines3.common.on_policy_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026df46",
   "metadata": {},
   "source": [
    "Progressbar callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb56b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import ProgressBarCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab64d48",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9aa97c",
   "metadata": {},
   "source": [
    "DQN make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c61fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import MlpPolicy, DQNPolicy, QNetwork\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077ce16",
   "metadata": {},
   "source": [
    "Below, we modify environment so we can use it for DQN !!! Delete it if it is not necessary!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb68863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(x_train, y_train), random=True):\n",
    "        # Actions we can take, classify as malicious or non-malicious\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        \n",
    "        # All the features we have - make sure LENGTH matches your feature count\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=float('-inf'), \n",
    "            high=float('inf'), \n",
    "            shape=(LENGTH,),\n",
    "            dtype=np.float32  # Specify dtype for consistency\n",
    "        )\n",
    "        \n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "        self.expected_action = 0  # Initialize this\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Calculate reward\n",
    "        reward = float(action == self.expected_action)  # Convert to float\n",
    "        current_label = self.expected_action\n",
    "        \n",
    "        # Get next observation\n",
    "        obs = self._next_obs()\n",
    "        \n",
    "        # Update step count\n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= self.images_per_episode\n",
    "        \n",
    "        # Standard gym format: obs, reward, done, info\n",
    "        info = {\n",
    "            'label': current_label,\n",
    "            'TimeLimit.truncated': done and self.step_count >= self.images_per_episode\n",
    "        }\n",
    "        \n",
    "        return obs, reward, done, info, {}\n",
    "    \n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y.iloc[next_obs_idx])\n",
    "            obs = self.x.iloc[next_obs_idx].values  # Convert to numpy array\n",
    "        else:\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                # Reset to beginning if we've reached the end\n",
    "                self.dataset_idx = 0\n",
    "            \n",
    "            obs = self.x.iloc[self.dataset_idx].values  # Convert to numpy array\n",
    "            self.expected_action = int(self.y.iloc[self.dataset_idx])\n",
    "            self.dataset_idx += 1\n",
    "            \n",
    "        # Ensure the observation is the right shape and type\n",
    "        return obs.astype(np.float32)\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.dataset_idx = 0 if not self.random else self.dataset_idx\n",
    "        \n",
    "        obs = self._next_obs()\n",
    "        info = {}  # Return empty dict for info\n",
    "        \n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    policy='MlpPolicy',#CustomPolicy\n",
    "    env=env,\n",
    "    gamma=1, #Discount factor\n",
    "    learning_rate=0.00025,  #Progress decreases from 1 to 0 -> lr decreasesb from 0.0021 to 0\n",
    "    # n_steps =512, #The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization) See\n",
    "    max_grad_norm=0.8, #The maximum value for the gradient clipping\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"log_25\", #tensorboard log location\n",
    "    # gae_lambda=0.8, #Factor for trade-off of bias vs variance for Generalized Advantage Estimato\n",
    "    batch_size=32, #minibatch size\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.1,\n",
    "    train_freq=4,\n",
    "    learning_starts=5000,\n",
    "    # buffer_size=1000000,\n",
    "    buffer_size = 1000\n",
    "    # target_network_update_freq=10000,\n",
    "    #normalize_advantage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3301a",
   "metadata": {},
   "source": [
    "the default policy depends on algorithm and observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = AccF1Callback(df_train, df_test, eval_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=5000000, #total number of samples(environment steps) to train on\n",
    "    callback=eval_callback,progress_bar=True,\n",
    "    # log_interval = int(1.0e4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54eec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.random(4).apply(predict_random_sample, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0910e",
   "metadata": {},
   "source": [
    "Feature Extractor redefined with net_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2604ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=32)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: torch.Tensor):\n",
    "        # raise ValueError(\"forward method was called\")\n",
    "        return self.network(observations)\n",
    "\n",
    "\n",
    "class CustomMlpPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        features_extractor_class = CustomFeatureExtractor\n",
    "        net_arch = [64,5]\n",
    "        super(CustomMlpPolicy, self).__init__(*args, **kwargs,net_arch = net_arch, features_extractor_class=features_extractor_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d454d",
   "metadata": {},
   "source": [
    "Batch Normaliztion in 1d with net_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfda6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=32)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: torch.Tensor):\n",
    "        # raise ValueError(\"forward method was called\")\n",
    "        return self.network(observations)\n",
    "\n",
    "\n",
    "class CustomMlpPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        features_extractor_class = CustomFeatureExtractor\n",
    "        net_arch = [64,5]\n",
    "        super(CustomMlpPolicy, self).__init__(*args, **kwargs,net_arch = net_arch, features_extractor_class=features_extractor_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6379a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a5db0",
   "metadata": {},
   "source": [
    "TabTransform in Feature Extractor with net_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=32)\n",
    "        self.embedding = nn.Linear(observation_space.shape[0], 32)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, observations: torch.Tensor):\n",
    "        x = self.embedding(observations)\n",
    "        x = x.unsqueeze(1) \n",
    "        x = self.transformer(x)\n",
    "        x = torch.mean(x, dim=1)  \n",
    "        return x\n",
    "\n",
    "class CustomMlpPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        features_extractor_class = CustomFeatureExtractor\n",
    "        net_arch = [64,5]\n",
    "        super(CustomMlpPolicy, self).__init__(*args, **kwargs,net_arch = net_arch, features_extractor_class=features_extractor_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643b18f",
   "metadata": {},
   "source": [
    "Redefine Policy Network in Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQNetwork(QNetwork):\n",
    "  def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Discrete,\n",
    "        features_extractor: BaseFeaturesExtractor,\n",
    "        features_dim: int,\n",
    "        net_arch: Optional[list[int]] = None,\n",
    "        activation_fn: type[nn.Module] = nn.ReLU,\n",
    "        normalize_images: bool = True,\n",
    "    ) -> None:\n",
    "        super(CustomQNetwork,self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            features_dim=features_dim,\n",
    "            features_extractor=features_extractor,\n",
    "            normalize_images=normalize_images,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # self.net_arch = net_arch\n",
    "        # self.activation_fn = activation_fn\n",
    "        # self.features_dim = features_dim\n",
    "        # action_dim = int(self.action_space.n)  # number of actions\n",
    "        # q_net = create_mlp(self.features_dim, action_dim, self.net_arch, self.activation_fn)\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(32, 16), nn.ReLU(),\n",
    "            nn.Linear(16 , 8), nn.ReLU(),\n",
    "            nn.Linear(8,2), nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=32)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: torch.Tensor):\n",
    "        # raise ValueError(\"forward method was called\")\n",
    "        return self.network(observations)\n",
    "\n",
    "\n",
    "class CustomMlpPolicy(DQNPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        features_extractor_class = CustomFeatureExtractor\n",
    "        super(CustomMlpPolicy, self).__init__(*args, **kwargs, features_extractor_class=features_extractor_class)\n",
    "    def make_q_net(self):\n",
    "       net_args = self._update_features_extractor(self.net_args)\n",
    "       return CustomQNetwork(**net_args).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb004af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_progress_bar()\n",
    "model.learn(\n",
    "    total_timesteps=100000, #total number of samples(environment steps) to train on\n",
    "    callback=eval_callback,progress_bar=True,\n",
    "    # log_interval = int(1.0e4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = df_test.apply(predict_row, axis=1)\n",
    "accuracy = accuracy_score(list(df_test[\"labels\"]), predicted.to_list())\n",
    "f1 = f1_score(list(df_test[\"labels\"]), predicted.to_list())\n",
    "print(\"Validation --- Accuracy: \", accuracy)\n",
    "print(\"Validation --- F1-Score: \", f1)\n",
    "\n",
    "print(\"*-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0003c",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64398fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d625d6a",
   "metadata": {},
   "source": [
    "It combines the idea of TRPO (using a trust region to ensure after each upadte, the new policy doesn't go far from the old one) and A2C (multiple agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e4f9c",
   "metadata": {},
   "source": [
    "##### Default Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy='MlpPolicy',#CustomPolicy\n",
    "    env=env,\n",
    "    gamma=0.9, #Discount factor\n",
    "    learning_rate=lambda progress: progress * 0.0021,  #Progress decreases from 1 to 0 -> lr decreasesb from 0.0021 to 0\n",
    "    n_steps=512, #The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization) See\n",
    "    ent_coef=1e-05, #Entropy coefficient for the loss calculation\n",
    "    vf_coef=0.6, #Value function coefficient for the loss calculation\n",
    "    max_grad_norm=0.8, #The maximum value for the gradient clipping\n",
    "    clip_range=0.2, #Clipping parameter, it can be a function of the current progress remaining (from 1 to 0)\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"log_25\", #tensorboard log location\n",
    "    gae_lambda=0.8, #Factor for trade-off of bias vs variance for Generalized Advantage Estimato\n",
    "    batch_size=16, #minibatch size\n",
    "    n_epochs=55 #Number of epoch when optimizing the surrogate loss\n",
    "    #normalize_advantage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f76107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=5000000, #total number of samples(environment steps) to train on\n",
    "    callback=eval_callback,progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4af740",
   "metadata": {},
   "source": [
    "Subprocess run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6920e4",
   "metadata": {},
   "source": [
    "PPO is meant to be run primarily on the CPU, especially when you are not using a CNN. To improve CPU utilization, try turning off the GPU and using **SubprocVecEnv** instead of the default **DummyVecEnv**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, device=\"cpu\")\n",
    "model.learn(total_timesteps=25_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7eaf1",
   "metadata": {},
   "source": [
    "Save/Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2103f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole\")\n",
    "model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5601b",
   "metadata": {},
   "source": [
    "#### Custom Policy Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[122, 64, 32],\n",
    "                     # features_extractor_class=\"MlpExtractor\",\n",
    "                     # features_extractor_kwargs=dict(features_dim=122)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy='MlpPolicy',#CustomPolicy\n",
    "    env=env,\n",
    "    gamma=0.9, #Discount factor\n",
    "    learning_rate=lambda progress: progress * 0.0021,  #Progress decreases from 1 to 0 -> lr decreasesb from 0.0021 to 0\n",
    "    n_steps=512, #The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization) See\n",
    "    ent_coef=1e-05, #Entropy coefficient for the loss calculation\n",
    "    vf_coef=0.6, #Value function coefficient for the loss calculation\n",
    "    max_grad_norm=0.8, #The maximum value for the gradient clipping\n",
    "    clip_range=0.2, #Clipping parameter, it can be a function of the current progress remaining (from 1 to 0)\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"log_25\", #tensorboard log location\n",
    "    gae_lambda=0.8, #Factor for trade-off of bias vs variance for Generalized Advantage Estimato\n",
    "    batch_size=16, #minibatch size\n",
    "    n_epochs=55, #Number of epoch when optimizing the surrogate loss\n",
    "    policy_kwargs=policy_kwargs\n",
    "    #normalize_advantage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419534e5",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd26696",
   "metadata": {},
   "source": [
    "A synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C). It uses multiple workers to avoid the use of a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = AccF1Callback(df_train, df_test, eval_freq=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e486f",
   "metadata": {},
   "source": [
    "### Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57744eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\"MlpPolicy\",\n",
    "            env=env, \n",
    "            gamma=0.9, #Discount factor\n",
    "            learning_rate=lambda progress: progress * 0.0021,  #Progress decreases from 1 to 0 -> lr decreasesb from 0.0021 to 0\n",
    "            n_steps=512, #The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization) See\n",
    "            ent_coef=1e-05, #Entropy coefficient for the loss calculation\n",
    "            vf_coef=0.6, #Value function coefficient for the loss calculation\n",
    "            max_grad_norm=0.8, #The maximum value for the gradient clipping\n",
    "            verbose=0,\n",
    "            tensorboard_log=\"log_25\", #tensorboard log location\n",
    "            gae_lambda=0.8, #Factor for trade-off of bias vs variance for Generalized Advantage Estimato\n",
    "            #normalize_advantage\n",
    "            device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_progress_bar()\n",
    "model.learn(\n",
    "    total_timesteps=100000, #total number of samples(environment steps) to train on\n",
    "    callback=eval_callback,progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2129845",
   "metadata": {},
   "source": [
    "### Vectorized Environment (Multi-Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c343dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from typing import Callable\n",
    "from stable_baselines3.common.utils import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75701df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(rank: int, seed: int = 0) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "\n",
    "    def _init() -> gym.Env:\n",
    "        # ev = gym.make(env)\n",
    "        ev = IdsEnv()\n",
    "        ev.reset(seed=seed + rank)\n",
    "        return ev\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67dcd0d",
   "metadata": {},
   "source": [
    "#### DummyVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "envv = DummyVecEnv([make_env(i) for i in range(num_cpu)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be63452",
   "metadata": {},
   "source": [
    "#### SubprocVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "import random\n",
    "from typing import Optional, Callable\n",
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1,dataset=(x_train, y_train), random=True):\n",
    "        self.action_space = gym.spaces.Discrete(2) ## normal or malicious\n",
    "        self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(len(x_train.columns),))\n",
    "        self.step_count = 0\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "        current_label = self.expected_action\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {} ,{'label': current_label}\n",
    "\n",
    "        ###### for algorithms with replay buffer as bellow\n",
    "        # info = {\n",
    "        #     'label': current_label,\n",
    "        #     'TimeLimit.truncated': done and self.step_count >= self.images_per_episode\n",
    "        # }\n",
    "        # return obs, reward, done, info, {}\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y.iloc[next_obs_idx,:])\n",
    "            obs = self.x.iloc[next_obs_idx,:]\n",
    "\n",
    "        else:\n",
    "            obs = self.x.iloc[self.dataset_idx]\n",
    "            self.expected_action = int(self.y.iloc[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "        return th.as_tensor(obs,device=\"cpu\")\n",
    "\n",
    "    def reset(self,seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "\n",
    "        self.step_count = 0\n",
    "        obs = self._next_obs()\n",
    "        return obs,{0:\"info\"}\n",
    "env = IdsEnv()\n",
    "# check_env(env)\n",
    "env.reset()\n",
    "# If you want to train the algorithm on multiple environment in parallel through the stable-baselines lib (vectorized environments)\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# n_envs = 16  # hyperparameter\n",
    "# env = DummyVecEnv([lambda: IdsEnv()] * n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "envv = SubprocVecEnv([make_env(i) for i in range(num_cpu)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\"MlpPolicy\",\n",
    "            env=env, \n",
    "            gamma=0.9, #Discount factor\n",
    "            learning_rate=lambda progress: progress * 0.0021,  #Progress decreases from 1 to 0 -> lr decreasesb from 0.0021 to 0\n",
    "            n_steps=512, #The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization) See\n",
    "            ent_coef=1e-05, #Entropy coefficient for the loss calculation\n",
    "            vf_coef=0.6, #Value function coefficient for the loss calculation\n",
    "            max_grad_norm=0.8, #The maximum value for the gradient clipping\n",
    "            verbose=0,\n",
    "            tensorboard_log=\"log_25\", #tensorboard log location\n",
    "            gae_lambda=0.8, #Factor for trade-off of bias vs variance for Generalized Advantage Estimato\n",
    "            #normalize_advantage\n",
    "            device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314087b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_progress_bar()\n",
    "model.learn(\n",
    "    total_timesteps=100000, #total number of samples(environment steps) to train on\n",
    "    callback=eval_callback,progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "envv.processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "envv.num_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc03f6",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = IdsEnv()\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = df_test.apply(predict_row, axis=1)\n",
    "accuracy = accuracy_score(list(df_test[\"labels\"]), predicted.to_list())\n",
    "f1 = f1_score(list(df_test[\"labels\"]), predicted.to_list())\n",
    "print(\"Validation --- Accuracy: \", accuracy)\n",
    "print(\"Validation --- F1-Score: \", f1)\n",
    "            \n",
    "print(\"*-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fb7db",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447fc00",
   "metadata": {},
   "source": [
    "#### Predict random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_sample(s):\n",
    "    print(s.labels.to_numpy(dtype=np.int64))\n",
    "    y = torch.tensor(s[\"labels\"].to_numpy(dtype=np.float32), dtype=torch.float)\n",
    "    f = torch.tensor(s.drop(columns=[\"labels\"]).to_numpy(dtype=np.float32), dtype=torch.float)\n",
    "    predicted = model.predict(f,deteministic=True)[0]\n",
    "    print(y, predicted)\n",
    "    print(\"Acc: \",accuracy_score(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c381c",
   "metadata": {},
   "source": [
    "#### Clear progressbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f470c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tqdm\n",
    "import rich\n",
    "\n",
    "def clear_progress_bar():\n",
    "    # On cherche tous les objets dont le nom du type contient tqdm\n",
    "    tqdm_objects = [obj for obj in gc.get_objects() if 'tqdm' in type(obj).__name__]\n",
    "    \n",
    "    # On affiche leur type\n",
    "    for tqdm_object in tqdm_objects:\n",
    "        print(type(tqdm_object).__name__)\n",
    "    \n",
    "    # On ferme ceux qu'on veut\n",
    "    for tqdm_object in tqdm_objects:\n",
    "        if 'tqdm_rich' in type(tqdm_object).__name__:\n",
    "            try:\n",
    "                tqdm_object.close()\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db0acf",
   "metadata": {},
   "source": [
    "#### Gym Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948b7fe",
   "metadata": {},
   "source": [
    "Train/Test Evalution on Frequency Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbe90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe956a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(row):\n",
    "            features = torch.tensor(row.drop(\"labels\").to_numpy(dtype=np.float32), dtype=torch.float32)\n",
    "            f =  model.predict(features, deterministic=True)[0]\n",
    "            return f\n",
    "    \n",
    "class AccF1Callback(BaseCallback):\n",
    "    def __init__(self,train, val, eval_freq,verbose:int=0 ):\n",
    "        super().__init__(verbose)\n",
    "        self.train_data = train\n",
    "        self.val_data = val\n",
    "        self.eval_freq = eval_freq\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseAlgorithm\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env # type: VecEnv\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # num_timesteps = n_envs * number of times env.step() was called\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "    \n",
    "    # def _on_training_start(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This method is called before the first rollout starts.\n",
    "    #     \"\"\"\n",
    "    #     pass\n",
    "\n",
    "    \n",
    "    # def _on_training_end(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This event is triggered before exiting the `learn()` method.\n",
    "    #     \"\"\"\n",
    "    #     pass\n",
    "    \n",
    "\n",
    "    # def _on_rollout_start(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     A rollout is the collection of environment interaction\n",
    "    #     using the current policy.\n",
    "    #     This event is triggered before collecting new samples.\n",
    "    #     \"\"\"\n",
    "    #     pass\n",
    "\n",
    "    # def _on_rollout_end(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This event is triggered before updating the policy.\n",
    "    #     \"\"\"\n",
    "    #     pass\n",
    "\n",
    "    \n",
    "        \n",
    "    def _on_step(self):\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "\n",
    "            predicted = self.train_data.apply(predict_row, axis=1)\n",
    "            accuracy = accuracy_score(list(self.train_data[\"labels\"]), predicted.to_list())\n",
    "            f1 = f1_score(list(self.train_data[\"labels\"]), predicted.to_list())\n",
    "\n",
    "            print(\"*-\" * 50)\n",
    "            \n",
    "            print(\"total current timesteps: \", self.num_timesteps, '\\n')\n",
    "            print(\"Training --- Accuracy: \", accuracy)\n",
    "            print(\"Training --- F1-Score: \", f1, '\\n')\n",
    "\n",
    "            predicted = self.val_data.apply(predict_row, axis=1)\n",
    "            accuracy = accuracy_score(list(self.val_data[\"labels\"]), predicted.to_list())\n",
    "            f1 = f1_score(list(self.val_data[\"labels\"]), predicted.to_list())\n",
    "            print(\"Validation --- Accuracy: \", accuracy)\n",
    "            print(\"Validation --- F1-Score: \", f1)\n",
    "            \n",
    "            print(\"*-\" * 50)\n",
    "\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
